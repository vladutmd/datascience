{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unprocessed_data = pd.read_csv('Data/cc_usage.csv')\n",
    "information = {'BALANCE':\"Available balance left on the card to make purchases (continuous)\",\n",
    "               'BALANCE_FREQUENCY': \"The frequency of updating the balance: 0 to 1 where \\\n",
    "                                     1 indicates frequent and 0 indicates not frequent (continuous)\",\n",
    "               'PURCHASES': \"Amount of purchases made by that credit card (continuous)\",\n",
    "               'ONEOFF_PURCHASES': \"Highest one-time purchase value (continuous)\",\n",
    "               'INSTALLMENTS_PURCHASES': \"Value of purchases paid for in installments (continuous)\",\n",
    "               'CASH_ADVANCE': \"Advance cash given by the customer\",\n",
    "               'PURCHASES_FREQUENCY': \"Frequency of purchases: 0 to 1 where 1 indicates frequent \\\n",
    "                                       and 0 indicates not frequent (continuous)\",\n",
    "               'ONEOFF_PURCHASES_FREQUENCY': \"Frequency of one-time purchases (not installments): 0 to 1 \\\n",
    "                                               where 1 indicates frequent and 0 indicates not frequent (continuous)\",\n",
    "               'PURCHASES_INSTALLMENTS_FREQUENCY': \"Frequency of purchases made in installments: 0 to 1 \\\n",
    "                                               where 1 indicates frequent and 0 indicates not frequent (continuous)\",\n",
    "               'CASH_ADVANCE_FREQUENCY': \"Frequency of cash advances: 0 to 1 where 1 indicates frequent \\\n",
    "                                       and 0 indicates not frequent (continuous)\",\n",
    "               'CASH_ADVANCE_TRX': \"Number of transactions made with 'cash in advance'\",\n",
    "               'PURCHASES_TRX': \"Number of purchase transactions made\",\n",
    "               'CREDIT_LIMIT': \"The credit limit of the customer\",\n",
    "               'PAYMENTS': \"Total amount of payments done by a customer\",\n",
    "               'MINIMUM_PAYMENTS': \"The amount of minimum payments amde by a customer\",\n",
    "               'PRC_FULL_PAYMENT': \"Percentage of the full credit card payment made by a customer\",\n",
    "               'TENURE': \"Tenure of the credit card service of a customer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ways to cluster the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I will discuss the common ways to cluster data and the ones that will be used in this study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-Means** clustering (assumes the dataset consists of spherical datasets), so the clusters of data observations created will be circular around the centroids. The centroids represent the \"centers\" of each cluster and are simply the mean of all the data points within the sample. K-means uses a distance (which one it is can be defined) and it requires the number of clusters to be know. The clusters are found by minimising **inertia** (i.e. **within-cluster sum-of-squares**). So K-means can produce inaccurate clusters. If the clusters are elongated, K-means would not work correctly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-shift\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the mean-shift algorithm, it also tries to find cluster centroids. The number of clusters, however, is not provided beforehand so the algorithm looks for *blobs* in the data that potentially be the cluster centroids. The candidates are updated (the near-duplicates are removed) until a final set of centroids is formed and the change in centroids is small. Since the algorithms looks for blobs, the clusters tend to be relatively isotropic but the assumption is not as strong as in the K-means algorithm. The problem with mean-shift it that it is not very scalable due to longer running/computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[DBSCAN](https://www.datanovia.com/en/lessons/dbscan-density-based-clustering-essentials/) is a density-clustering dataset. It works by finding *dense* regions in the dataset. If many data points are close together, it is a *high-density* region. If sparse, it is a *low-density* region. The algorithm treats the high density regions as clusters and the low-density regions as the *noise* between the clusters. Consequently, if there are outliers, the DBSCAN algorithm would not include them in the clusters but instead leave them as noise. Similarly to the mean-shift algorith, DBSCAN automatically determines the number of clusters but it is also highly scalable and in makes no assumptions about the shape of the clusters (they can be any shape). Please note that DBSCAN can be used to [find outliers](\n",
    "https://www.datasciencelearner.com/projection-detecting-outliers-dbscan-method/  ) in the data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical-clustering \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative to K-means clustering that makes no assumptions about the shape of the clusters is [hierarchical clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering) and can therefore be used to cluster any type of data. There are two types of hierarchical clustering: top-down or bottom-up. In the former approach (top-down/divisive), all the data is initially treated as one single cluster and it is then repeteadly split into smaller clusters until the desired number of clusters is reached. In the bottom-up approach (agglomerative) all the data points are initially treated as separate clusters and they are then grouped until a defined stopping criteria is met. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deciding on the number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We mentioned that for K-means clustering and Hierarhical clustering the number of clusters should be specified. How does one decide on the number? There are a few commonly used methods (can be considered direct and statistical testing methods):\n",
    "- elbow method (direct method)  \n",
    "- silhouette score (direct method)  \n",
    "- gap statistic method (statistical testing method)  \n",
    "\n",
    "What is a direct method?\n",
    "A direct method consists of optimising some criteria (for example, the inertia or the average silhouette).\n",
    "\n",
    "What is a statistical testing method?\n",
    "It consists of comparing some evidence against the null hypothesis.\n",
    "\n",
    "Other methods do [exist](https://www.datanovia.com/en/lessons/determining-the-optimal-number-of-clusters-3-must-know-methods/) and may be added later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow method is one of the most popular methods. For an increasing number of clusters, the inertia is calculated and plotted against the number of clusters k. Where there is a change in the slope from steep to shallow (a bend/knee/elbow), that value is then used as the optimal number of clusters. The method can, however, be ambiguous but is still used a lot and potentially helpful. It will be explored in the current study.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Average) Silhouette Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this method the [average silhouette](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py) of the observations is computed for different values of K. The optimal number of clusters is the one that will maximise the average silhouette. What is, however, a silhouette plot? It's a measure of how close points in one cluster are to points in the neighbouring clusters. Silhouette coefficients close to +1 indicate that those data points are far away from the neighbouring clusters and coefficients close to -1 indicate that the data points might have been assigned to an incorrect cluster. A value of 0 is on or in the proximity of the decision bounfdary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gap statistic method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [gap statistic method](http://web.stanford.edu/~hastie/Papers/gap.pdf), as mentioned earlier, is a statistical testing method. It compares the sum of all the variations within each of the clusters for all the clusters for various values of k and compares that sum with mean values obtained from an appropriate reference null distribution. The higher the gap statistic the more optimal the number of clusters is.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
